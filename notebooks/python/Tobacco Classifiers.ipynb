{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will load three different classifiers to label tweets hierarchically at 3 different levels. First, they will be sorted into behavior-related vs. not related. Second, they will be classified into 1st person vs. not 1st person. Third, a label will be assigned to first-person, behavior-related tweets that show time-related activity in the past, present, or future. \n",
    "\n",
    "First, we load the requisite modules into the environmment and change our base directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sklearn\n",
    "import pandas\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/dethf/NYU/Spring 2019/Chunara Internship/nyu-twipsy/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the pickle files which contain our classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tob = pickle.load(open('/Users/dethf/NYU/Spring 2019/Chunara Internship/nyu-research/tobacco/tobacco-classifiers/classifier1_tobacco.p', 'rb'))\n",
    "clf_fpt = pickle.load(open('/Users/dethf/NYU/Spring 2019/Chunara Internship/nyu-research/tobacco/tobacco-classifiers/classifier2_firstPerson.p', 'rb'))\n",
    "clf_fpl = pickle.load(open('/Users/dethf/NYU/Spring 2019/Chunara Internship/nyu-research/tobacco/tobacco-classifiers/classifier3_present.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the threshold: 99% of data is not alcohol related\n",
    "# JM Comment: this did not seem to be needed with alcohol tobacco, will try without this step to see how it goes... \n",
    "#clf_tob.steps[2][1].class_weight = {0:0.99, 1:0.01}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use some pre-classified tweets and regenerate the tobacco-related vs. not tobacco-related, first-person vs. not first-person, and first-person-level labels. Since the data were used to train the classifier initially and were hand-annotated by MTurk workers, We should not expect to come up with identical labels for these levels of classification.\n",
    "\n",
    "There are multiple training datasets for each level of classification. We will load them all below. Then, we will see how many of each label there are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb = pandas.read_csv(\"C:/Users/dethf/NYU/Spring 2019/Chunara Internship/nyu-research/tobacco/training-data/combined.csv\")\n",
    "df_tob = pandas.read_csv(\"C:/Users/dethf/NYU/Spring 2019/Chunara Internship/nyu-research/tobacco/training-data/tob.csv\")\n",
    "df_1p = pandas.read_csv(\"C:/Users/dethf/NYU/Spring 2019/Chunara Internship/nyu-research/tobacco/training-data/fp.csv\")\n",
    "df_pres = pandas.read_csv(\"C:/Users/dethf/NYU/Spring 2019/Chunara Internship/nyu-research/tobacco/training-data/present.csv\")\n",
    "df_curr = pandas.read_csv(\"C:/Users/dethf/NYU/Spring 2019/Chunara Internship/nyu-research/tobacco/training-data/current.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT-Tobacco: 22492 Tobacco: 3676\n"
     ]
    }
   ],
   "source": [
    "# printing number of tobacco vs. not tobacco tweets labeled\n",
    "print(\"NOT-Tobacco:\", numpy.unique(df_tob.labels, return_counts=True)[1][0], \n",
    "      \"Tobacco:\", numpy.unique(df_tob.labels, return_counts=True)[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are 22492 tweets labeled not tobacco related while 3676 were tobacco related. We next run  the classifier on this prelabeled dataset to see how many of the original labels we can reproduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the classifier\n",
    "tob_preds = clf_tob.predict(df_tob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT-Tobacco: 22492 Tobacco: 3676\n"
     ]
    }
   ],
   "source": [
    "# finding out how many tweets there were in each group\n",
    "print(\"NOT-Tobacco:\", numpy.unique(tob_preds, return_counts=True)[1][0],\n",
    "      \"Tobacco:\",numpy.unique(tob_preds, return_counts=True)[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the same number of tweets per group, but we will now check to see if there were any mislabeled tweets and check the predicted probabilities for either label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5161</th>\n",
       "      <td>NOT-Tobacco</td>\n",
       "      <td>Do u even vape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24820</th>\n",
       "      <td>Tobacco</td>\n",
       "      <td>Who tryna smoke</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            labels             text\n",
       "5161   NOT-Tobacco   Do u even vape\n",
       "24820      Tobacco  Who tryna smoke"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tob[tob_preds != df_tob.labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tob_probs = clf_tob.predict_proba(df_tob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48639826, 0.51360174],\n",
       "       [0.51083009, 0.48916991]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tob_probs[numpy.where(tob_preds != df_tob.labels)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the prediction probabilities that it was nearly a toss-up as to the label those tweets got. Those tweets were clearly difficult to categorize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only two tweets mislabeled out of over 20,000 is remarkably accurate.\n",
    "\n",
    "Next we will run the first person tobacco classifier on the tweets used for its training to see how many of the original labels we can reproduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running classifier\n",
    "fp_preds = clf_fpt.predict(df_1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT-1stPerson: 1628 1stPersont: 2048\n"
     ]
    }
   ],
   "source": [
    "# printing number of first-person vs. not first person tweets annotated (by hand)\n",
    "print(\"NOT-1stPerson:\", numpy.unique(df_1p.labels, return_counts=True)[1][0], \n",
    "      \"1stPersont:\", numpy.unique(df_1p.labels, return_counts=True)[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT-1stPerson: 1626 1stPerson: 2050\n"
     ]
    }
   ],
   "source": [
    "# printing number of first person vs. not first person tweets the classifier predicted\n",
    "print(\"NOT-1stPerson:\", numpy.unique(fp_preds, return_counts=True)[1][0], \n",
    "      \"1stPerson:\", numpy.unique(fp_preds, return_counts=True)[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't generate the same label distribution. We check next to see what tweets were mislabeled and their prediction probabilities as we did above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>1stPerson</td>\n",
       "      <td>I hate cigarettes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2176</th>\n",
       "      <td>1stPerson</td>\n",
       "      <td>look at this cool vape trick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         labels                          text\n",
       "1399  1stPerson             I hate cigarettes\n",
       "2176  1stPerson  look at this cool vape trick"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1p[fp_preds != df_1p.labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48797202, 0.51202798],\n",
       "       [0.48560346, 0.51439654]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_probs = clf_fpt.predict_proba(df_1p)\n",
    "fp_probs[numpy.where(fp_preds != df_1p.labels)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the two mislabeled tweets seem clearly difficult to categorize as either first person or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now continue with the next level of classification: present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the classifier\n",
    "pres_preds = clf_fpl.predict(df_pres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check to see if our classification reproduced the original annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT-Present: 662 Present: 966\n"
     ]
    }
   ],
   "source": [
    "# printing number of present vs. not present tweets annotated (by hand)\n",
    "print(\"NOT-Present:\", numpy.unique(df_pres.labels, return_counts=True)[1][0], \n",
    "      \"Present:\", numpy.unique(df_pres.labels, return_counts=True)[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT-Present: 660 Present: 968\n"
     ]
    }
   ],
   "source": [
    "# printing number of present vs. not present tweets classified\n",
    "print(\"NOT-Present:\", numpy.unique(pres_preds, return_counts=True)[1][0], \n",
    "      \"Present:\", numpy.unique(pres_preds, return_counts=True)[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we do not generate the same label distribution. We check next to see what tweets were misclassified and what their prediction probabilities were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>NOT-Present</td>\n",
       "      <td>Hookah bar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>NOT-Present</td>\n",
       "      <td>I got a project in mind so wine glass and ciga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          labels                                               text\n",
       "455  NOT-Present                                         Hookah bar\n",
       "540  NOT-Present  I got a project in mind so wine glass and ciga..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pres[pres_preds != df_pres.labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19836836, 0.80163164],\n",
       "       [0.49771675, 0.50228325]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pres_probs = clf_fpl.predict_proba(df_pres)\n",
    "pres_probs[numpy.where(pres_preds != df_pres.labels)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, remarkably, there are only two mistakes made by the classifier; both tweets are mislabeled as NOT-Present. Only the second tweet was difficult to categorize. The first tweet, \"Hookah bar,\" was given a more obvious classification, but its short length may have contributed to this somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-0a476c46ea81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconf_mat_pres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpres_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_pres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_mat_pres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "conf_mat_pres = confusion_matrix(pres_preds, df_pres.labels)\n",
    "print(conf_mat_pres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have finished running our classifiers, but there still remains the \"Current\" training data which are the same tweets used to train the first person level classifier. Since the existing classifiers will not return any 'current' labels, we will not be able to test their accuracy. However, we know from the Tom Huang paper that 'present' and 'habitual' posts were combined. It may be that these combined tweets were then given the label 'Current.' We will attempt to figure that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the classifier\n",
    "curr_preds = clf_fpl.predict(df_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['current', 'not-current'], dtype=object),\n",
       " array([1398,  230], dtype=int64))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.unique(df_curr.labels, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['NOT-Present', 'Present'], dtype=object),\n",
       " array([662, 966], dtype=int64))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.unique(df_pres.labels, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are 1398 tweets labeled current and 230 not current. We also see again that there are 966 tweets labeled Present, and 662 tweets labeled NOT-Present. If all the 'Present' tweets also share the label 'Current,' they must be a subset of the entire 'Current' tweets. We will check this now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "966"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_curr.labels[numpy.where(df_pres.labels == \"Present\")[0]] == \"current\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suspected, the 'Current' label is just the 'Present' label and probably a 'Habitual' one that were combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below the 1398 tweets labeled current above are in the final 'Combined' dataset, representing the combined labels at each level of classification: Tobacco, First Person, and Current. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.unique(df_comb.labels, return_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
